{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, iText2KG is compatible with all language models supported by LangChain. \n",
    "\n",
    "To use iText2KG, you will need both a chat model and an embeddings model. \n",
    "\n",
    "For available chat models, refer to the options listed at: https://python.langchain.com/v0.2/docs/integrations/chat/. \n",
    "For embedding models, explore the choices at: https://python.langchain.com/v0.2/docs/integrations/text_embedding/. \n",
    "\n",
    "This notebook will show you how to run iText2KG using Mistral, Ollama, and OpenAI models. \n",
    "\n",
    "**Please ensure that you install the necessary package for each chat model before use.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mistral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Mistral, please set up your model using the tutorial here: https://python.langchain.com/v0.2/docs/integrations/chat/mistralai/. Similarly, for the embedding model, follow the setup guide here: https://python.langchain.com/v0.2/docs/integrations/text_embedding/mistralai/ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_mistralai'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_mistralai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatMistralAI\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_mistralai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MistralAIEmbeddings\n\u001b[32m      4\u001b[39m mistral_api_key = \u001b[33m\"\u001b[39m\u001b[33m##\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain_mistralai'"
     ]
    }
   ],
   "source": [
    "from langchain_mistralai import ChatMistralAI\n",
    "from langchain_mistralai import MistralAIEmbeddings\n",
    "\n",
    "mistral_api_key = \"##\"\n",
    "mistral_llm_model = ChatMistralAI(\n",
    "    api_key = mistral_api_key,\n",
    "    model=\"mistral-large-latest\",\n",
    "    temperature=0,\n",
    "    max_retries=2,\n",
    ")\n",
    "\n",
    "\n",
    "mistral_embeddings_model = MistralAIEmbeddings(\n",
    "    model=\"mistral-embed\",\n",
    "    api_key = mistral_api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same applies for OpenAI. \n",
    "\n",
    "please setup your model using the tutorial : https://python.langchain.com/v0.2/docs/integrations/chat/openai/\n",
    "The same for embedding model : https://python.langchain.com/v0.2/docs/integrations/text_embedding/openai/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Client.__init__() got an unexpected keyword argument 'proxies'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_openai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI, OpenAIEmbeddings\n\u001b[32m      3\u001b[39m openai_api_key = \u001b[33m\"\u001b[39m\u001b[33msk-proj-i1-2nYDeUikOl2STTY3om1e-wf0UHs5JGm_smDjoLw-3ER-8Pxa5zvIZfZOBkPETyrVGNPj4WRT3BlbkFJLQXzEsP2nnfFxOHGlmPmJpVF0Rp2WL6--789fueaPJ66vVHyaNvImHwoGNkfcqsX2FroVYIyoA\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m openai_llm_model = llm = \u001b[43mChatOpenAI\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mopenai_api_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgpt-4o\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m openai_embeddings_model = OpenAIEmbeddings(\n\u001b[32m     15\u001b[39m     api_key = openai_api_key ,\n\u001b[32m     16\u001b[39m     model=\u001b[33m\"\u001b[39m\u001b[33mtext-embedding-3-large\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     17\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\soft\\anaconda3\\envs\\itext2kgenv\\Lib\\site-packages\\langchain_core\\load\\serializable.py:125\u001b[39m, in \u001b[36mSerializable.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    124\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\soft\\anaconda3\\envs\\itext2kgenv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:516\u001b[39m, in \u001b[36mBaseChatOpenAI.validate_environment\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    514\u001b[39m         \u001b[38;5;28mself\u001b[39m.http_client = httpx.Client(proxy=\u001b[38;5;28mself\u001b[39m.openai_proxy)\n\u001b[32m    515\u001b[39m     sync_specific = {\u001b[33m\"\u001b[39m\u001b[33mhttp_client\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.http_client}\n\u001b[32m--> \u001b[39m\u001b[32m516\u001b[39m     \u001b[38;5;28mself\u001b[39m.root_client = \u001b[43mopenai\u001b[49m\u001b[43m.\u001b[49m\u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mclient_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msync_specific\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    517\u001b[39m     \u001b[38;5;28mself\u001b[39m.client = \u001b[38;5;28mself\u001b[39m.root_client.chat.completions\n\u001b[32m    518\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.async_client:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\soft\\anaconda3\\envs\\itext2kgenv\\Lib\\site-packages\\openai\\_client.py:123\u001b[39m, in \u001b[36mOpenAI.__init__\u001b[39m\u001b[34m(self, api_key, organization, project, base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m base_url \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    121\u001b[39m     base_url = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mhttps://api.openai.com/v1\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mversion\u001b[49m\u001b[43m=\u001b[49m\u001b[43m__version__\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhttp_client\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhttp_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_strict_response_validation\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_strict_response_validation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[38;5;28mself\u001b[39m._default_stream_cls = Stream\n\u001b[32m    136\u001b[39m \u001b[38;5;28mself\u001b[39m.completions = resources.Completions(\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\soft\\anaconda3\\envs\\itext2kgenv\\Lib\\site-packages\\openai\\_base_client.py:844\u001b[39m, in \u001b[36mSyncAPIClient.__init__\u001b[39m\u001b[34m(self, version, base_url, max_retries, timeout, transport, proxies, limits, http_client, custom_headers, custom_query, _strict_response_validation)\u001b[39m\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    828\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid `http_client` argument; Expected an instance of `httpx.Client` but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(http_client)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    829\u001b[39m     )\n\u001b[32m    831\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\n\u001b[32m    832\u001b[39m     version=version,\n\u001b[32m    833\u001b[39m     limits=limits,\n\u001b[32m   (...)\u001b[39m\u001b[32m    842\u001b[39m     _strict_response_validation=_strict_response_validation,\n\u001b[32m    843\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m844\u001b[39m \u001b[38;5;28mself\u001b[39m._client = http_client \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mSyncHttpxClientWrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    846\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# cast to a valid type because mypy doesn't understand our type narrowing\u001b[39;49;00m\n\u001b[32m    847\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    848\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    849\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransport\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    850\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlimits\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlimits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    851\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    852\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\soft\\anaconda3\\envs\\itext2kgenv\\Lib\\site-packages\\openai\\_base_client.py:742\u001b[39m, in \u001b[36m_DefaultHttpxClient.__init__\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    740\u001b[39m kwargs.setdefault(\u001b[33m\"\u001b[39m\u001b[33mlimits\u001b[39m\u001b[33m\"\u001b[39m, DEFAULT_CONNECTION_LIMITS)\n\u001b[32m    741\u001b[39m kwargs.setdefault(\u001b[33m\"\u001b[39m\u001b[33mfollow_redirects\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m742\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: Client.__init__() got an unexpected keyword argument 'proxies'"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "openai_api_key = \"your_api_key\"\n",
    "\n",
    "openai_llm_model = ChatOpenAI(\n",
    "    api_key = openai_api_key,\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "\n",
    "openai_embeddings_model = OpenAIEmbeddings(\n",
    "    api_key = openai_api_key ,\n",
    "    model=\"text-embedding-3-large\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ZhipuAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatZhipuAI\n",
    "\n",
    "\n",
    "# 设置 API key（推荐用环境变量）\n",
    "zhipu_api_key = \"your_api_key\"\n",
    "\n",
    "# 初始化智谱大语言模型\n",
    "zhipu_llm_model = ChatZhipuAI(\n",
    "    api_key=zhipu_api_key,\n",
    "    model=\"glm-4\",  # 可选模型：glm-3-turbo / glm-4 / glm-4v\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same applies for Ollama. \n",
    "\n",
    "please setup your model using the tutorial : https://python.langchain.com/v0.2/docs/integrations/chat/ollama/\n",
    "The same for embedding model : https://python.langchain.com/v0.2/docs/integrations/text_embedding/openai/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3\",\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=\"llama3\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# iText2KG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use case: we aim to connect two scientific papers. \n",
    "\n",
    "* The objective is to detect common key concepts between the two papers and allowing for the identification of central themes, keywords, and topics that dominate each paper. These themes could be linked to show overlaps or gaps in coverage, helping researchers identify areas where more study might be needed or where novel connections could be made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Distiller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scientific articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from itext2kg.documents_distiller import DocumentsDistiller, Article\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "class ArticleResults(BaseModel):\n",
    "    abstract:str = Field(description=\"Brief summary of the article's abstract\")\n",
    "    key_findings:str = Field(description=\"The key findings of the article\")\n",
    "    limitation_of_sota : str=Field(description=\"limitation of the existing work\")\n",
    "    proposed_solution : str = Field(description=\"the proposed solution in details\")\n",
    "    paper_limitations : str=Field(description=\"The limitations of the proposed solution of the paper\")\n",
    "\n",
    "# Sample input data as a list of triplets\n",
    "# It is structured in this manner : (document's path, page_numbers_to_exclude, blueprint, document_type)\n",
    "documents_information = [\n",
    "    (\"../datasets/llm-tikg.pdf\", [11,10], ArticleResults, 'scientific article'),\n",
    "    (\"../datasets/actionable-cyber-threat.pdf\", [12,11,10], ArticleResults, 'scientific article')\n",
    "]\n",
    "\n",
    "\"\"\"\n",
    "将每个文档根据提示词蒸馏得到distilled_doc,\n",
    "最终返回distilled_docs\n",
    "\"\"\"\n",
    "\n",
    "def upload_and_distill(documents_information: List[Tuple[str, List[int], BaseModel]]):\n",
    "    distilled_docs = []\n",
    "    \n",
    "    for path_, exclude_pages, blueprint, document_type in documents_information:\n",
    "        \n",
    "        loader = PyPDFLoader(path_)\n",
    "        pages = loader.load_and_split()\n",
    "        pages = [page for page in pages if page.metadata[\"page\"]+1 not in exclude_pages] # Exclude some pages (unecessary pages, for example, the references)\n",
    "        document_distiller = DocumentsDistiller(llm_model=llm)\n",
    "        \n",
    "        IE_query = f'''\n",
    "        # DIRECTIVES : \n",
    "        - Act like an experienced information extractor.\n",
    "        - You have a chunk of a {document_type}\n",
    "        - If you do not find the right information, keep its place empty.\n",
    "        '''\n",
    "        \n",
    "        # Distill document content with query\n",
    "        distilled_doc = document_distiller.distill(\n",
    "            documents=[page.page_content.replace(\"{\", '[').replace(\"}\", \"]\") for page in pages],\n",
    "            IE_query=IE_query,\n",
    "            output_data_structure=blueprint\n",
    "        )\n",
    "        \n",
    "        # Filter and format distilled document results\n",
    "        distilled_docs.append([\n",
    "            f\"{document_type}'s {key} - {value}\".replace(\"{\", \"[\").replace(\"}\", \"]\") \n",
    "            for key, value in distilled_doc.items() \n",
    "            if value and value != []\n",
    "        ])\n",
    "    \n",
    "    return distilled_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distilled documents saved to distilled_docs.md\n"
     ]
    }
   ],
   "source": [
    "def my_upload_and_distill(documents_information: List[Tuple[str, List[int], BaseModel]]):\n",
    "    distilled_docs = []\n",
    "    \n",
    "    for path_, exclude_pages, blueprint, document_type in documents_information:\n",
    "        \n",
    "        loader = PyPDFLoader(path_)\n",
    "        pages = loader.load_and_split()\n",
    "        pages = [page for page in pages if page.metadata[\"page\"]+1 not in exclude_pages] # Exclude some pages (unecessary pages, for example, the references)\n",
    "        document_distiller = DocumentsDistiller(llm_model=llm)\n",
    "        \n",
    "        IE_query = f'''\n",
    "        # DIRECTIVES : \n",
    "        - Act like an experienced information extractor.\n",
    "        - You have a chunk of a {document_type}\n",
    "        - If you do not find the right information, keep its place empty.\n",
    "        '''\n",
    "        \n",
    "        # Distill document content with query\n",
    "        distilled_doc = document_distiller.distill(\n",
    "            documents=[page.page_content.replace(\"{\", '[').replace(\"}\", \"]\") for page in pages],\n",
    "            IE_query=IE_query,\n",
    "            output_data_structure=blueprint\n",
    "        )\n",
    "        \n",
    "        # Filter and format distilled document results\n",
    "        distilled_docs.append([\n",
    "            f\"{document_type}'s {key} - {value}\".replace(\"{\", \"[\").replace(\"}\", \"]\") \n",
    "            for key, value in distilled_doc.items() \n",
    "            if value and value != []\n",
    "        ])\n",
    "    \n",
    "    return distilled_docs\n",
    "\n",
    "my_documents_information = [\n",
    "    (\"../datasets/scientific_articles/llm-tikg.pdf\", [2,3,4,5,6,7,8,9,10,11], ArticleResults, 'scientific article')\n",
    "]\n",
    "\n",
    "my_distilled_docs = my_upload_and_distill(my_documents_information)\n",
    "\n",
    "# Save distilled_docs to a Markdown file\n",
    "with open(\"distilled_docs.md\", \"w\", encoding=\"utf-8\") as md_file:\n",
    "    md_file.write(\"# Distilled Documents\\n\\n\")\n",
    "    for doc in my_distilled_docs:\n",
    "        md_file.write(\"\\n\".join(doc) + \"\\n\\n\")\n",
    "\n",
    "print(\"Distilled documents saved to distilled_docs.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "distilled_docs = upload_and_distill(documents_information=documents_information)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iText2KG for graph construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itext2kg import iText2KG\n",
    "\n",
    "\n",
    "itext2kg = iText2KG(llm_model = llm, embeddings_model = embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We construct the first knowledge graph of the first distilled documents (for the first article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] ------- Extracting Entities from the Document 1\n",
      "[INFO] ------- Extracting Relations from the Document 1\n",
      "[INFO] Verification of invented entities\n",
      "[DEBUG] Processing relationship: {'startNode': {'label': 'Malware', 'name': 'identical malware techniques'}, 'endNode': {'label': 'Attackers', 'name': 'malware and attackers'}, 'name': 'employs'}\n",
      "[INFO] ------- Extracting Entities from the Document 2\n",
      "[INFO] ------- Extracting Relations from the Document 2\n",
      "[INFO] Verification of invented entities\n",
      "[DEBUG] Processing relationship: {'startNode': {'label': 'Key_Findings', 'name': 'The main results and discoveries presented in the article (second instance)'}, 'endNode': {'label': 'Proposed_Solution', 'name': 'The new approach, method, or technique proposed in the article to address the limitations of sota (second instance)'}, 'name': 'Addresses'}\n",
      "[DEBUG] Processing relationship: {'startNode': {'label': 'Limitation_of_SOTA', 'name': 'The limitations of the current state of the art (sota) methods or techniques (third instance)'}, 'endNode': {'label': 'Proposed_Solution', 'name': 'The new approach, method, or technique proposed in the article to address the limitations of sota'}, 'name': 'Improves'}\n",
      "[DEBUG] Processing relationship: {'startNode': {'label': 'Key_Findings', 'name': 'The main results and discoveries presented in the article (third instance)'}, 'endNode': {'label': 'Abstract', 'name': 'The summary of the scientific article (third instance)'}, 'name': 'Summarizes'}\n",
      "[DEBUG] Processing relationship: {'startNode': {'label': 'Limitation_of_SOTA', 'name': 'The limitations of the current state of the art (sota) methods or techniques'}, 'endNode': {'label': 'Proposed_Solution', 'name': 'The new approach, method, or technique proposed in the article to address the limitations of sota (third instance)'}, 'name': 'Overcomes'}\n",
      "[DEBUG] Processing relationship: {'startNode': {'label': 'Abstract', 'name': 'The summary of the scientific article'}, 'endNode': {'label': 'Key_Findings', 'name': 'The main results and discoveries presented in the article'}, 'name': 'Introduces'}\n",
      "[INFO][ISOLATED ENTITIES][TRY-1] Aie; there are some isolated entities without relations [Entity(name=the limitations and potential biases of the scientific paper itself (third instance), label=Paper_Limitations, properties=embeddings=array([ 0.00306729, -0.01676326,  0.00840163, ..., -0.00051094,\n",
      "       -0.00765459,  0.00545748])), Entity(name=the limitations and potential biases of the scientific paper itself, label=Paper_Limitations, properties=embeddings=array([-0.00235787, -0.01186482,  0.00620327, ..., -0.00760922,\n",
      "       -0.00091248,  0.00040684])), Entity(name=the limitations of the current state of the art (sota) methods or techniques (second instance), label=Limitation_of_SOTA, properties=embeddings=array([ 0.00278721, -0.01132383,  0.00234306, ...,  0.00421733,\n",
      "       -0.00294726,  0.00271445])), Entity(name=the limitations and potential biases of the scientific paper itself (second instance), label=Paper_Limitations, properties=embeddings=array([ 0.00011367, -0.01739178,  0.00863068, ...,  0.00021098,\n",
      "       -0.00485381,  0.00655582])), Entity(name=the summary of the scientific article (second instance), label=Abstract, properties=embeddings=array([-0.00267938, -0.01813502,  0.00573875, ...,  0.01089128,\n",
      "       -0.00635099,  0.00133149]))]. Solving them ...\n",
      "[INFO] Verification of invented entities\n",
      "[DEBUG] Processing relationship: {'startNode': {'label': 'Paper_Limitations', 'name': 'the limitations and potential biases of the scientific paper itself'}, 'endNode': {'label': '', 'name': 'the limitations and potential biases of the scientific paper itself (third instance)'}, 'name': 'same_as'}\n",
      "[INFO][INVENTED ENTITIES] Aie; the entities label='' name='the limitations and potential biases of the scientific paper itself (third instance)' properties=EntityProperties(embeddings=None) is invented. Solving it ...\n",
      "[INFO] Wohoo! Entity was matched --- [the limitations and potential biases of the scientific paper itself (third instance):] --merged--> [the limitations and potential biases of the scientific paper itself (third instance):Paper_Limitations]\n",
      "[DEBUG] Processing relationship: {'startNode': {'label': 'Limitation_of_SOTA', 'name': 'the limitations of the current state of the art (sota) methods or techniques'}, 'endNode': {'label': '', 'name': 'the limitations of the current state of the art (sota) methods or techniques (second instance)'}, 'name': 'same_as'}\n",
      "[INFO][INVENTED ENTITIES] Aie; the entities label='Limitation_of_SOTA' name='the limitations of the current state of the art (sota) methods or techniques' properties=EntityProperties(embeddings=None) and label='' name='the limitations of the current state of the art (sota) methods or techniques (second instance)' properties=EntityProperties(embeddings=None) are invented. Solving them ...\n",
      "[INFO] Wohoo! Entity was matched --- [the limitations of the current state of the art (sota) methods or techniques:Limitation_of_SOTA] --merged--> [the limitations and potential biases of the scientific paper itself:Paper_Limitations]\n",
      "[INFO] Wohoo! Entity was matched --- [the limitations of the current state of the art (sota) methods or techniques (second instance):] --merged--> [the limitations of the current state of the art (sota) methods or techniques (second instance):Limitation_of_SOTA]\n",
      "[DEBUG] Processing relationship: {'startNode': {'label': 'Abstract', 'name': 'the summary of the scientific article'}, 'endNode': {'label': '', 'name': 'the summary of the scientific article (second instance)'}, 'name': 'same_as'}\n",
      "[INFO][INVENTED ENTITIES] Aie; the entities label='Abstract' name='the summary of the scientific article' properties=EntityProperties(embeddings=None) and label='' name='the summary of the scientific article (second instance)' properties=EntityProperties(embeddings=None) are invented. Solving them ...\n",
      "[INFO] Wohoo! Entity was matched --- [the summary of the scientific article:Abstract] --merged--> [the summary of the scientific article (second instance):Abstract]\n",
      "[INFO] Wohoo! Entity was matched --- [the summary of the scientific article (second instance):] --merged--> [the summary of the scientific article (second instance):Abstract]\n",
      "[INFO][ISOLATED ENTITIES][TRY-2] Aie; there are some isolated entities without relations [Entity(name=the main results and discoveries presented in the article (second instance), label=Key_Findings, properties=embeddings=array([ 1.66823858e-03, -2.61814040e-02,  3.23464270e-03, ...,\n",
      "        6.26674716e-03,  1.33086600e-05,  4.21593060e-03])), Entity(name=the new approach, method, or technique proposed in the article to address the limitations of sota (second instance), label=Proposed_Solution, properties=embeddings=array([-0.00695101, -0.01784235, -0.00175366, ..., -0.00428499,\n",
      "       -0.00324738,  0.00199496])), Entity(name=the main results and discoveries presented in the article, label=Key_Findings, properties=embeddings=array([ 0.00054416, -0.02166802,  0.00083475, ..., -0.00296124,\n",
      "        0.0024659 , -0.00276609])), Entity(name=the limitations of the current state of the art (sota) methods or techniques (third instance), label=Limitation_of_SOTA, properties=embeddings=array([ 0.00406293, -0.00597988,  0.00468485, ...,  0.00810701,\n",
      "       -0.00396405,  0.00055871])), Entity(name=the new approach, method, or technique proposed in the article to address the limitations of sota, label=Proposed_Solution, properties=embeddings=array([-0.01327418, -0.0078127 , -0.0036682 , ..., -0.00847725,\n",
      "        0.00112295, -0.0023187 ])), Entity(name=the limitations of the current state of the art (sota) methods or techniques, label=Limitation_of_SOTA, properties=embeddings=array([-0.00995269,  0.00412207,  0.00284313, ...,  0.00389266,\n",
      "       -0.00688865, -0.00743503])), Entity(name=the main results and discoveries presented in the article (third instance), label=Key_Findings, properties=embeddings=array([ 0.00207128, -0.02368785,  0.00457606, ...,  0.00494586,\n",
      "        0.0009503 ,  0.00370105])), Entity(name=the new approach, method, or technique proposed in the article to address the limitations of sota (third instance), label=Proposed_Solution, properties=embeddings=array([-0.00751831, -0.02149304,  0.00209201, ..., -0.00030875,\n",
      "       -0.0026061 , -0.00040806])), Entity(name=the summary of the scientific article, label=Abstract, properties=embeddings=array([-0.00593816, -0.0084407 ,  0.00682398, ...,  0.00598778,\n",
      "       -0.00437109, -0.00577408])), Entity(name=the summary of the scientific article (third instance), label=Abstract, properties=embeddings=array([-0.00250682, -0.01642548,  0.00501686, ...,  0.01155692,\n",
      "       -0.00545071,  0.00087896])), Entity(name=the limitations and potential biases of the scientific paper itself (second instance), label=Paper_Limitations, properties=embeddings=array([ 0.00011367, -0.01739178,  0.00863068, ...,  0.00021098,\n",
      "       -0.00485381,  0.00655582]))]. Solving them ...\n",
      "[INFO] Verification of invented entities\n",
      "[DEBUG] Processing relationship: {'startNode': {'label': 'the main results and discoveries presented in the article (second instance)', 'name': 'Second Instance Key Findings'}, 'endNode': {'label': 'Key_Findings', 'name': 'Key Findings'}, 'name': 'Key Findings'}\n",
      "[INFO][INVENTED ENTITIES] Aie; the entities label='the_main_results_and_discoveries_presented_in_the_article__second_instance_' name='second instance key findings' properties=EntityProperties(embeddings=None) and label='Key_Findings' name='key findings' properties=EntityProperties(embeddings=None) are invented. Solving them ...\n",
      "[INFO] Wohoo! Entity was matched --- [second instance key findings:the_main_results_and_discoveries_presented_in_the_article__second_instance_] --merged--> [the new approach, method, or technique proposed in the article to address the limitations of sota (second instance):Proposed_Solution]\n",
      "[INFO] Wohoo! Entity was matched --- [key findings:Key_Findings] --merged--> [the main results and discoveries presented in the article:Key_Findings]\n",
      "[DEBUG] Processing relationship: {'startNode': {'label': 'the new approach, method, or technique proposed in the article to address the limitations of sota (second instance)', 'name': 'Second Instance Proposed Solution'}, 'endNode': {'label': 'Proposed_Solution', 'name': 'Proposed Solution'}, 'name': 'Proposed Solution'}\n",
      "[INFO][INVENTED ENTITIES] Aie; the entities label='the_new_approach__method__or_technique_proposed_in_the_article_to_address_the_limitations_of_sota__second_instance_' name='second instance proposed solution' properties=EntityProperties(embeddings=None) and label='Proposed_Solution' name='proposed solution' properties=EntityProperties(embeddings=None) are invented. Solving them ...\n",
      "[INFO] Wohoo! Entity was matched --- [second instance proposed solution:the_new_approach__method__or_technique_proposed_in_the_article_to_address_the_limitations_of_sota__second_instance_] --merged--> [the new approach, method, or technique proposed in the article to address the limitations of sota (second instance):Proposed_Solution]\n",
      "[INFO] Wohoo! Entity was matched --- [proposed solution:Proposed_Solution] --merged--> [the new approach, method, or technique proposed in the article to address the limitations of sota (second instance):Proposed_Solution]\n",
      "[DEBUG] Processing relationship: {'startNode': {'label': 'the main results and discoveries presented in the article', 'name': 'Main Results Key Findings'}, 'endNode': {'label': 'Key_Findings', 'name': 'Key Findings'}, 'name': 'Key Findings'}\n",
      "[INFO][INVENTED ENTITIES] Aie; the entities label='the_main_results_and_discoveries_presented_in_the_article' name='main results key findings' properties=EntityProperties(embeddings=None) and label='Key_Findings' name='key findings' properties=EntityProperties(embeddings=None) are invented. Solving them ...\n",
      "[INFO] Wohoo! Entity was matched --- [main results key findings:the_main_results_and_discoveries_presented_in_the_article] --merged--> [the main results and discoveries presented in the article:Key_Findings]\n",
      "[INFO] Wohoo! Entity was matched --- [key findings:Key_Findings] --merged--> [the main results and discoveries presented in the article:Key_Findings]\n",
      "[DEBUG] Processing relationship: {'startNode': {'label': 'the limitations of the current state of the art (sota) methods or techniques (third instance)', 'name': 'Third Instance Limitation of SOTA'}, 'endNode': {'label': 'Limitation_of_SOTA', 'name': 'Limitation of SOTA'}, 'name': 'Limitation of SOTA'}\n",
      "[INFO][INVENTED ENTITIES] Aie; the entities label='the_limitations_of_the_current_state_of_the_art__sota__methods_or_techniques__third_instance_' name='third instance limitation of sota' properties=EntityProperties(embeddings=None) and label='Limitation_of_SOTA' name='limitation of sota' properties=EntityProperties(embeddings=None) are invented. Solving them ...\n",
      "[INFO] Wohoo! Entity was matched --- [third instance limitation of sota:the_limitations_of_the_current_state_of_the_art__sota__methods_or_techniques__third_instance_] --merged--> [the new approach, method, or technique proposed in the article to address the limitations of sota (second instance):Proposed_Solution]\n",
      "[INFO] Wohoo! Entity was matched --- [limitation of sota:Limitation_of_SOTA] --merged--> [the new approach, method, or technique proposed in the article to address the limitations of sota:Proposed_Solution]\n",
      "[DEBUG] Processing relationship: {'startNode': {'label': 'the new approach, method, or technique proposed in the article to address the limitations of sota', 'name': 'Proposed Solution'}, 'endNode': {'label': 'Proposed_Solution', 'name': 'Proposed Solution'}, 'name': 'Proposed Solution'}\n",
      "[INFO][INVENTED ENTITIES] Aie; the entities label='the_new_approach__method__or_technique_proposed_in_the_article_to_address_the_limitations_of_sota' name='proposed solution' properties=EntityProperties(embeddings=None) and label='Proposed_Solution' name='proposed solution' properties=EntityProperties(embeddings=None) are invented. Solving them ...\n",
      "[INFO] Wohoo! Entity was matched --- [proposed solution:the_new_approach__method__or_technique_proposed_in_the_article_to_address_the_limitations_of_sota] --merged--> [the new approach, method, or technique proposed in the article to address the limitations of sota:Proposed_Solution]\n",
      "[INFO] Wohoo! Entity was matched --- [proposed solution:Proposed_Solution] --merged--> [the new approach, method, or technique proposed in the article to address the limitations of sota (second instance):Proposed_Solution]\n",
      "[DEBUG] Processing relationship: {'startNode': {'label': 'the main results and discoveries presented in the article (third instance)', 'name': 'Third Instance Key Findings'}, 'endNode': {'label': 'Key_Findings', 'name': 'Key Findings'}, 'name': 'Key Findings'}\n",
      "[INFO][INVENTED ENTITIES] Aie; the entities label='the_main_results_and_discoveries_presented_in_the_article__third_instance_' name='third instance key findings' properties=EntityProperties(embeddings=None) and label='Key_Findings' name='key findings' properties=EntityProperties(embeddings=None) are invented. Solving them ...\n",
      "[INFO] Wohoo! Entity was matched --- [third instance key findings:the_main_results_and_discoveries_presented_in_the_article__third_instance_] --merged--> [the new approach, method, or technique proposed in the article to address the limitations of sota (second instance):Proposed_Solution]\n",
      "[INFO] Wohoo! Entity was matched --- [key findings:Key_Findings] --merged--> [the main results and discoveries presented in the article:Key_Findings]\n",
      "[DEBUG] Processing relationship: {'startNode': {'label': 'the new approach, method, or technique proposed in the article to address the limitations of sota (third instance)', 'name': 'Third Instance Proposed Solution'}, 'endNode': {'label': 'Proposed_Solution', 'name': 'Proposed Solution'}, 'name': 'Proposed Solution'}\n",
      "[INFO][INVENTED ENTITIES] Aie; the entities label='the_new_approach__method__or_technique_proposed_in_the_article_to_address_the_limitations_of_sota__third_instance_' name='third instance proposed solution' properties=EntityProperties(embeddings=None) and label='Proposed_Solution' name='proposed solution' properties=EntityProperties(embeddings=None) are invented. Solving them ...\n",
      "[INFO] Wohoo! Entity was matched --- [third instance proposed solution:the_new_approach__method__or_technique_proposed_in_the_article_to_address_the_limitations_of_sota__third_instance_] --merged--> [the new approach, method, or technique proposed in the article to address the limitations of sota (second instance):Proposed_Solution]\n",
      "[INFO] Wohoo! Entity was matched --- [proposed solution:Proposed_Solution] --merged--> [the new approach, method, or technique proposed in the article to address the limitations of sota (second instance):Proposed_Solution]\n",
      "[DEBUG] Processing relationship: {'startNode': {'label': 'the summary of the scientific article', 'name': 'Abstract'}, 'endNode': {'label': 'Abstract', 'name': 'Abstract'}, 'name': 'Abstract'}\n",
      "[INFO][INVENTED ENTITIES] Aie; the entities label='the_summary_of_the_scientific_article' name='abstract' properties=EntityProperties(embeddings=None) and label='Abstract' name='abstract' properties=EntityProperties(embeddings=None) are invented. Solving them ...\n",
      "[INFO] Wohoo! Entity was matched --- [abstract:Abstract] --merged--> [the summary of the scientific article:Abstract]\n",
      "[DEBUG] Processing relationship: {'startNode': {'label': 'the limitations and potential biases of the scientific paper itself (second instance)', 'name': 'Second Instance Paper Limitations'}, 'endNode': {'label': 'Paper_Limitations', 'name': 'Paper Limitations'}, 'name': 'Paper Limitations'}\n",
      "[INFO][INVENTED ENTITIES] Aie; the entities label='the_limitations_and_potential_biases_of_the_scientific_paper_itself__second_instance_' name='second instance paper limitations' properties=EntityProperties(embeddings=None) and label='Paper_Limitations' name='paper limitations' properties=EntityProperties(embeddings=None) are invented. Solving them ...\n",
      "[INFO] Wohoo! Entity was matched --- [second instance paper limitations:the_limitations_and_potential_biases_of_the_scientific_paper_itself__second_instance_] --merged--> [the new approach, method, or technique proposed in the article to address the limitations of sota (second instance):Proposed_Solution]\n",
      "[INFO] Wohoo! Entity was matched --- [paper limitations:Paper_Limitations] --merged--> [the limitations and potential biases of the scientific paper itself (second instance):Paper_Limitations]\n",
      "[INFO][ISOLATED ENTITIES][TRY-3] Aie; there are some isolated entities without relations [Entity(name=the main results and discoveries presented in the article (second instance), label=Key_Findings, properties=embeddings=array([ 1.66823858e-03, -2.61814040e-02,  3.23464270e-03, ...,\n",
      "        6.26674716e-03,  1.33086600e-05,  4.21593060e-03])), Entity(name=the limitations and potential biases of the scientific paper itself (third instance), label=Paper_Limitations, properties=embeddings=array([ 0.00306729, -0.01676326,  0.00840163, ..., -0.00051094,\n",
      "       -0.00765459,  0.00545748])), Entity(name=the limitations of the current state of the art (sota) methods or techniques (third instance), label=Limitation_of_SOTA, properties=embeddings=array([ 0.00406293, -0.00597988,  0.00468485, ...,  0.00810701,\n",
      "       -0.00396405,  0.00055871])), Entity(name=the limitations of the current state of the art (sota) methods or techniques, label=Limitation_of_SOTA, properties=embeddings=array([-0.00995269,  0.00412207,  0.00284313, ...,  0.00389266,\n",
      "       -0.00688865, -0.00743503])), Entity(name=the main results and discoveries presented in the article (third instance), label=Key_Findings, properties=embeddings=array([ 0.00207128, -0.02368785,  0.00457606, ...,  0.00494586,\n",
      "        0.0009503 ,  0.00370105])), Entity(name=the limitations and potential biases of the scientific paper itself, label=Paper_Limitations, properties=embeddings=array([-0.00235787, -0.01186482,  0.00620327, ..., -0.00760922,\n",
      "       -0.00091248,  0.00040684])), Entity(name=the limitations of the current state of the art (sota) methods or techniques (second instance), label=Limitation_of_SOTA, properties=embeddings=array([ 0.00278721, -0.01132383,  0.00234306, ...,  0.00421733,\n",
      "       -0.00294726,  0.00271445])), Entity(name=the new approach, method, or technique proposed in the article to address the limitations of sota (third instance), label=Proposed_Solution, properties=embeddings=array([-0.00751831, -0.02149304,  0.00209201, ..., -0.00030875,\n",
      "       -0.0026061 , -0.00040806])), Entity(name=the summary of the scientific article (third instance), label=Abstract, properties=embeddings=array([-0.00250682, -0.01642548,  0.00501686, ...,  0.01155692,\n",
      "       -0.00545071,  0.00087896])), Entity(name=the summary of the scientific article (second instance), label=Abstract, properties=embeddings=array([-0.00267938, -0.01813502,  0.00573875, ...,  0.01089128,\n",
      "       -0.00635099,  0.00133149]))]. Solving them ...\n",
      "[INFO] Verification of invented entities\n",
      "[DEBUG] Processing relationship: {'startNode': {'label': 'the main results and discoveries presented in the article (second instance)', 'name': 'Second Instance Key Findings'}, 'endNode': {'label': 'Key_Findings', 'name': 'Key Findings'}, 'name': 'Key Findings'}\n",
      "[INFO][INVENTED ENTITIES] Aie; the entities label='the_main_results_and_discoveries_presented_in_the_article__second_instance_' name='second instance key findings' properties=EntityProperties(embeddings=None) and label='Key_Findings' name='key findings' properties=EntityProperties(embeddings=None) are invented. Solving them ...\n",
      "[INFO] Wohoo! Entity was matched --- [second instance key findings:the_main_results_and_discoveries_presented_in_the_article__second_instance_] --merged--> [the summary of the scientific article (third instance):Abstract]\n",
      "[INFO] Wohoo! Entity was matched --- [key findings:Key_Findings] --merged--> [the main results and discoveries presented in the article (third instance):Key_Findings]\n",
      "[DEBUG] Processing relationship: {'startNode': {'label': 'the limitations and potential biases of the scientific paper itself (third instance)', 'name': 'Third Instance Paper Limitations'}, 'endNode': {'label': 'Paper_Limitations', 'name': 'Paper Limitations'}, 'name': 'Paper Limitations'}\n",
      "[INFO][INVENTED ENTITIES] Aie; the entities label='the_limitations_and_potential_biases_of_the_scientific_paper_itself__third_instance_' name='third instance paper limitations' properties=EntityProperties(embeddings=None) and label='Paper_Limitations' name='paper limitations' properties=EntityProperties(embeddings=None) are invented. Solving them ...\n",
      "[INFO] Wohoo! Entity was matched --- [third instance paper limitations:the_limitations_and_potential_biases_of_the_scientific_paper_itself__third_instance_] --merged--> [the summary of the scientific article (second instance):Abstract]\n",
      "[INFO] Wohoo! Entity was matched --- [paper limitations:Paper_Limitations] --merged--> [the limitations and potential biases of the scientific paper itself (third instance):Paper_Limitations]\n",
      "[DEBUG] Processing relationship: {'startNode': {'label': 'the limitations of the current state of the art (sota) methods or techniques (third instance)', 'name': 'Third Instance SOTA Limitations'}, 'endNode': {'label': 'Limitation_of_SOTA', 'name': 'Limitation of SOTA'}, 'name': 'Limitation of SOTA'}\n",
      "[INFO][INVENTED ENTITIES] Aie; the entities label='the_limitations_of_the_current_state_of_the_art__sota__methods_or_techniques__third_instance_' name='third instance sota limitations' properties=EntityProperties(embeddings=None) and label='Limitation_of_SOTA' name='limitation of sota' properties=EntityProperties(embeddings=None) are invented. Solving them ...\n",
      "[INFO] Wohoo! Entity was matched --- [third instance sota limitations:the_limitations_of_the_current_state_of_the_art__sota__methods_or_techniques__third_instance_] --merged--> [the summary of the scientific article (third instance):Abstract]\n",
      "[DEBUG] Processing relationship: {'startNode': {'label': 'the limitations of the current state of the art (sota) methods or techniques', 'name': 'SOTA Limitations'}, 'endNode': {'label': 'Limitation_of_SOTA', 'name': 'Limitation of SOTA'}, 'name': 'Limitation of SOTA'}\n",
      "[INFO][INVENTED ENTITIES] Aie; the entities label='the_limitations_of_the_current_state_of_the_art__sota__methods_or_techniques' name='sota limitations' properties=EntityProperties(embeddings=None) and label='Limitation_of_SOTA' name='limitation of sota' properties=EntityProperties(embeddings=None) are invented. Solving them ...\n",
      "[INFO] Wohoo! Entity was matched --- [sota limitations:the_limitations_of_the_current_state_of_the_art__sota__methods_or_techniques] --merged--> [the limitations of the current state of the art (sota) methods or techniques:Limitation_of_SOTA]\n",
      "[DEBUG] Processing relationship: {'startNode': {'label': 'the main results and discoveries presented in the article (third instance)', 'name': 'Third Instance Key Findings'}, 'endNode': {'label': 'Key_Findings', 'name': 'Key Findings'}, 'name': 'Key Findings'}\n",
      "[INFO][INVENTED ENTITIES] Aie; the entities label='the_main_results_and_discoveries_presented_in_the_article__third_instance_' name='third instance key findings' properties=EntityProperties(embeddings=None) and label='Key_Findings' name='key findings' properties=EntityProperties(embeddings=None) are invented. Solving them ...\n",
      "[INFO] Wohoo! Entity was matched --- [third instance key findings:the_main_results_and_discoveries_presented_in_the_article__third_instance_] --merged--> [the summary of the scientific article (third instance):Abstract]\n",
      "[INFO] Wohoo! Entity was matched --- [key findings:Key_Findings] --merged--> [the main results and discoveries presented in the article (third instance):Key_Findings]\n",
      "[DEBUG] Processing relationship: {'startNode': {'label': 'the limitations and potential biases of the scientific paper itself', 'name': 'Paper Limitations'}, 'endNode': {'label': 'Paper_Limitations', 'name': 'Paper Limitations'}, 'name': 'Paper Limitations'}\n",
      "[INFO][INVENTED ENTITIES] Aie; the entities label='the_limitations_and_potential_biases_of_the_scientific_paper_itself' name='paper limitations' properties=EntityProperties(embeddings=None) and label='Paper_Limitations' name='paper limitations' properties=EntityProperties(embeddings=None) are invented. Solving them ...\n",
      "[INFO] Wohoo! Entity was matched --- [paper limitations:the_limitations_and_potential_biases_of_the_scientific_paper_itself] --merged--> [the limitations and potential biases of the scientific paper itself:Paper_Limitations]\n",
      "[INFO] Wohoo! Entity was matched --- [paper limitations:Paper_Limitations] --merged--> [the limitations and potential biases of the scientific paper itself (third instance):Paper_Limitations]\n",
      "[DEBUG] Processing relationship: {'startNode': {'label': 'the limitations of the current state of the art (sota) methods or techniques (second instance)', 'name': 'Second Instance SOTA Limitations'}, 'endNode': {'label': 'Limitation_of_SOTA', 'name': 'Limitation of SOTA'}, 'name': 'Limitation of SOTA'}\n",
      "[INFO][INVENTED ENTITIES] Aie; the entities label='the_limitations_of_the_current_state_of_the_art__sota__methods_or_techniques__second_instance_' name='second instance sota limitations' properties=EntityProperties(embeddings=None) and label='Limitation_of_SOTA' name='limitation of sota' properties=EntityProperties(embeddings=None) are invented. Solving them ...\n",
      "[INFO] Wohoo! Entity was matched --- [second instance sota limitations:the_limitations_of_the_current_state_of_the_art__sota__methods_or_techniques__second_instance_] --merged--> [the summary of the scientific article (second instance):Abstract]\n",
      "[DEBUG] Processing relationship: {'startNode': {'label': 'the new approach, method, or technique proposed in the article to address the limitations of sota (third instance)', 'name': 'Third Instance Proposed Solution'}, 'endNode': {'label': 'Proposed_Solution', 'name': 'Proposed Solution'}, 'name': 'Proposed Solution'}\n",
      "[INFO][INVENTED ENTITIES] Aie; the entities label='the_new_approach__method__or_technique_proposed_in_the_article_to_address_the_limitations_of_sota__third_instance_' name='third instance proposed solution' properties=EntityProperties(embeddings=None) and label='Proposed_Solution' name='proposed solution' properties=EntityProperties(embeddings=None) are invented. Solving them ...\n",
      "[INFO] Wohoo! Entity was matched --- [third instance proposed solution:the_new_approach__method__or_technique_proposed_in_the_article_to_address_the_limitations_of_sota__third_instance_] --merged--> [the new approach, method, or technique proposed in the article to address the limitations of sota (third instance):Proposed_Solution]\n",
      "[INFO] Wohoo! Entity was matched --- [proposed solution:Proposed_Solution] --merged--> [the new approach, method, or technique proposed in the article to address the limitations of sota (third instance):Proposed_Solution]\n",
      "[DEBUG] Processing relationship: {'startNode': {'label': 'the summary of the scientific article (third instance)', 'name': 'Third Instance Abstract'}, 'endNode': {'label': 'Abstract', 'name': 'Abstract'}, 'name': 'Abstract'}\n",
      "[INFO][INVENTED ENTITIES] Aie; the entities label='the_summary_of_the_scientific_article__third_instance_' name='third instance abstract' properties=EntityProperties(embeddings=None) and label='Abstract' name='abstract' properties=EntityProperties(embeddings=None) are invented. Solving them ...\n",
      "[INFO] Wohoo! Entity was matched --- [third instance abstract:the_summary_of_the_scientific_article__third_instance_] --merged--> [the summary of the scientific article (third instance):Abstract]\n",
      "[INFO] Wohoo! Entity was matched --- [abstract:Abstract] --merged--> [the summary of the scientific article (third instance):Abstract]\n",
      "[DEBUG] Processing relationship: {'startNode': {'label': 'the summary of the scientific article (second instance)', 'name': 'Second Instance Abstract'}, 'endNode': {'label': 'Abstract', 'name': 'Abstract'}, 'name': 'Abstract'}\n",
      "[INFO][INVENTED ENTITIES] Aie; the entities label='the_summary_of_the_scientific_article__second_instance_' name='second instance abstract' properties=EntityProperties(embeddings=None) and label='Abstract' name='abstract' properties=EntityProperties(embeddings=None) are invented. Solving them ...\n",
      "[INFO] Wohoo! Entity was matched --- [second instance abstract:the_summary_of_the_scientific_article__second_instance_] --merged--> [the summary of the scientific article (third instance):Abstract]\n",
      "[INFO] Wohoo! Entity was matched --- [abstract:Abstract] --merged--> [the summary of the scientific article (third instance):Abstract]\n",
      "[INFO] ------- Extracting Entities from the Document 3\n",
      "[INFO] ------- Extracting Relations from the Document 3\n",
      "[INFO] Verification of invented entities\n",
      "[DEBUG] Processing relationship: {'startNode': {'label': 'Person', 'name': 'attackers'}, 'endNode': {'label': 'Methodology', 'name': 'social engineering'}, 'name': 'uses'}\n",
      "[DEBUG] Processing relationship: {'startNode': {'label': 'Person', 'name': 'attackers'}, 'endNode': {'label': 'Attack', 'name': 'spear phishing'}, 'name': 'performs'}\n",
      "[DEBUG] Processing relationship: {'startNode': {'label': 'Methodology', 'name': 'social engineering'}, 'endNode': {'label': 'Technique', 'name': 'phishing email'}, 'name': 'employs'}\n",
      "[INFO] Wohoo! Relation was matched --- [performs] --merged --> [employs] \n",
      "[INFO] ------- Extracting Entities from the Document 4\n",
      "[INFO] Wohoo! Entity was matched --- [acquiring knowledge about attacker behavioral traits:Attacker_Behavioral_Traits] --merged--> [the limitations and potential biases of the scientific paper itself:Paper_Limitations]\n",
      "[INFO] ------- Extracting Relations from the Document 4\n",
      "[INFO] Verification of invented entities\n",
      "[DEBUG] Processing relationship: {'startNode': {'label': 'Graph_Construction', 'name': 'Constructed Graph'}, 'endNode': {'label': 'Threat_Intelligence_Report', 'name': 'Threat Intelligence Report'}, 'name': 'Utilized for'}\n",
      "[INFO][INVENTED ENTITIES] Aie; the entities label='Graph_Construction' name='constructed graph' properties=EntityProperties(embeddings=None) and label='Threat_Intelligence_Report' name='threat intelligence report' properties=EntityProperties(embeddings=None) are invented. Solving them ...\n",
      "[INFO] Wohoo! Entity was matched --- [constructed graph:Graph_Construction] --merged--> [constructed graph for threat hunting and attack attribution:Graph_Construction]\n",
      "[INFO] Wohoo! Entity was matched --- [threat intelligence report:Threat_Intelligence_Report] --merged--> [9681 articles classified as threat intelligence reports:Threat_Intelligence_Report]\n",
      "[DEBUG] Processing relationship: {'startNode': {'label': 'Malicious_Samples', 'name': 'Malicious Samples'}, 'endNode': {'label': 'Graph_Construction', 'name': 'Constructed Graph'}, 'name': 'Identified from'}\n",
      "[INFO][INVENTED ENTITIES] Aie; the entities label='Malicious_Samples' name='malicious samples' properties=EntityProperties(embeddings=None) and label='Graph_Construction' name='constructed graph' properties=EntityProperties(embeddings=None) are invented. Solving them ...\n",
      "[INFO] Wohoo! Entity was matched --- [malicious samples:Malicious_Samples] --merged--> [identification of malicious samples for training models:Malicious_Samples]\n",
      "[INFO] Wohoo! Entity was matched --- [constructed graph:Graph_Construction] --merged--> [constructed graph for threat hunting and attack attribution:Graph_Construction]\n",
      "[DEBUG] Processing relationship: {'startNode': {'label': 'Threat_Intelligence_Report', 'name': 'Threat Intelligence Report'}, 'endNode': {'label': 'Graph_Construction', 'name': 'Constructed Graph'}, 'name': 'Classified as'}\n",
      "[INFO][INVENTED ENTITIES] Aie; the entities label='Threat_Intelligence_Report' name='threat intelligence report' properties=EntityProperties(embeddings=None) and label='Graph_Construction' name='constructed graph' properties=EntityProperties(embeddings=None) are invented. Solving them ...\n",
      "[INFO] Wohoo! Entity was matched --- [threat intelligence report:Threat_Intelligence_Report] --merged--> [9681 articles classified as threat intelligence reports:Threat_Intelligence_Report]\n",
      "[INFO] Wohoo! Entity was matched --- [constructed graph:Graph_Construction] --merged--> [constructed graph for threat hunting and attack attribution:Graph_Construction]\n",
      "[INFO][ISOLATED ENTITIES][TRY-1] Aie; there are some isolated entities without relations [Entity(name=the limitations and potential biases of the scientific paper itself, label=Paper_Limitations, properties=embeddings=array([-0.00235787, -0.01186482,  0.00620327, ..., -0.00760922,\n",
      "       -0.00091248,  0.00040684]))]. Solving them ...\n",
      "[INFO] Verification of invented entities\n",
      "[DEBUG] Processing relationship: {'startNode': {'label': 'Paper_Limitations', 'name': 'the limitations and potential biases of the scientific paper itself'}, 'endNode': {'label': 'Entity', 'name': ''}, 'name': 'has_limitation'}\n",
      "[INFO][INVENTED ENTITIES] Aie; the entities label='Entity' name='' properties=EntityProperties(embeddings=None) is invented. Solving it ...\n",
      "[INFO][ISOLATED ENTITIES][TRY-2] Aie; there are some isolated entities without relations [Entity(name=constructed graph for threat hunting and attack attribution, label=Graph_Construction, properties=embeddings=array([-0.01449408, -0.00585874,  0.00133084, ...,  0.0023727 ,\n",
      "        0.0095165 ,  0.00050827])), Entity(name=identification of malicious samples for training models, label=Malicious_Samples, properties=embeddings=array([-0.00141297, -0.00592055, -0.00720303, ...,  0.00369237,\n",
      "       -0.00193122,  0.00103799])), Entity(name=9681 articles classified as threat intelligence reports, label=Threat_Intelligence_Report, properties=embeddings=array([ 0.00273901, -0.01230786, -0.01374235, ...,  0.00376004,\n",
      "        0.00054061,  0.00045991]))]. Solving them ...\n",
      "[INFO] Verification of invented entities\n",
      "[DEBUG] Processing relationship: {'startNode': {'label': 'Graph Construction', 'name': 'Constructed Graph'}, 'endNode': {'label': 'Threat Intelligence Report', 'name': '9681 articles classified as threat intelligence reports'}, 'name': 'Graph Construction for Threat Hunting and Attack Attribution'}\n",
      "[INFO][INVENTED ENTITIES] Aie; the entities label='Graph_Construction' name='constructed graph' properties=EntityProperties(embeddings=None) is invented. Solving it ...\n",
      "[INFO] Wohoo! Entity was matched --- [constructed graph:Graph_Construction] --merged--> [constructed graph for threat hunting and attack attribution:Graph_Construction]\n",
      "[DEBUG] Processing relationship: {'startNode': {'label': 'Malicious Samples', 'name': 'Identification of malicious samples for training models'}, 'endNode': {'label': 'Threat Intelligence Report', 'name': '9681 articles classified as threat intelligence reports'}, 'name': 'Identification of Malicious Samples for Training Models'}\n",
      "[INFO][ISOLATED ENTITIES][TRY-3] Aie; there are some isolated entities without relations [Entity(name=the limitations and potential biases of the scientific paper itself, label=Paper_Limitations, properties=embeddings=array([-0.00235787, -0.01186482,  0.00620327, ..., -0.00760922,\n",
      "       -0.00091248,  0.00040684]))]. Solving them ...\n",
      "[INFO] Verification of invented entities\n",
      "[DEBUG] Processing relationship: {'startNode': {'label': 'Paper_Limitations', 'name': 'the limitations and potential biases of the scientific paper itself'}, 'endNode': {'label': 'Entity', 'name': 'the provided context'}, 'name': 'Describes'}\n",
      "[INFO][INVENTED ENTITIES] Aie; the entities label='Entity' name='the provided context' properties=EntityProperties(embeddings=None) is invented. Solving it ...\n",
      "[INFO] Wohoo! Relation was matched --- [Classified_as] --merged --> [same_as] \n",
      "[INFO] Wohoo! Relation was matched --- [has_limitation] --merged --> [same_as] \n",
      "[INFO] Wohoo! Relation was matched --- [Graph_Construction_for_Threat_Hunting_and_Attack_Attribution] --merged --> [Key_Findings] \n",
      "[INFO] Wohoo! Relation was matched --- [Identification_of_Malicious_Samples_for_Training_Models] --merged --> [Proposed_Solution] \n",
      "[INFO] Wohoo! Relation was matched --- [Describes] --merged --> [Summarizes] \n",
      "[INFO] ------- Extracting Entities from the Document 5\n",
      "[INFO] ------- Extracting Relations from the Document 5\n",
      "Error in parsing the instance context : --\n",
      "'scientific article's limitation_of_sota - Existing intrusion detection methods that rely solely on IoC values are ineffective in detecting attacks, as attackers often update their used IPs and domains from time to time.' \n",
      " entities :-- \n",
      " [('existing intrusion detection methods that rely solely on ioc values are ineffective in detecting attacks, as attackers often update their used ips and domains from time to time.', 'Limitation')]\n",
      "Error in parsing the instance context : --\n",
      "'scientific article's limitation_of_sota - Existing intrusion detection methods that rely solely on IoC values are ineffective in detecting attacks, as attackers often update their used IPs and domains from time to time.' \n",
      " entities :-- \n",
      " [('existing intrusion detection methods that rely solely on ioc values are ineffective in detecting attacks, as attackers often update their used ips and domains from time to time.', 'Limitation')]\n",
      "Error in parsing the instance context : --\n",
      "'scientific article's limitation_of_sota - Existing intrusion detection methods that rely solely on IoC values are ineffective in detecting attacks, as attackers often update their used IPs and domains from time to time.' \n",
      " entities :-- \n",
      " [('existing intrusion detection methods that rely solely on ioc values are ineffective in detecting attacks, as attackers often update their used ips and domains from time to time.', 'Limitation')]\n",
      "Error in parsing the instance context : --\n",
      "'scientific article's limitation_of_sota - Existing intrusion detection methods that rely solely on IoC values are ineffective in detecting attacks, as attackers often update their used IPs and domains from time to time.' \n",
      " entities :-- \n",
      " [('existing intrusion detection methods that rely solely on ioc values are ineffective in detecting attacks, as attackers often update their used ips and domains from time to time.', 'Limitation')]\n",
      "Error in parsing the instance context : --\n",
      "'scientific article's limitation_of_sota - Existing intrusion detection methods that rely solely on IoC values are ineffective in detecting attacks, as attackers often update their used IPs and domains from time to time.' \n",
      " entities :-- \n",
      " [('existing intrusion detection methods that rely solely on ioc values are ineffective in detecting attacks, as attackers often update their used ips and domains from time to time.', 'Limitation')]\n",
      "[INFO] Verification of invented entities\n",
      "[INFO][ISOLATED ENTITIES][TRY-1] Aie; there are some isolated entities without relations [Entity(name=existing intrusion detection methods that rely solely on ioc values are ineffective in detecting attacks, as attackers often update their used ips and domains from time to time., label=Limitation, properties=embeddings=array([ 0.00304498, -0.01894589,  0.00282755, ..., -0.00377315,\n",
      "       -0.00309917,  0.00512137]))]. Solving them ...\n",
      "[INFO] Verification of invented entities\n",
      "[DEBUG] Processing relationship: {'startNode': {'label': 'Limitation', 'name': 'Existing intrusion detection methods that rely solely on IoC values are ineffective in detecting attacks, as attackers often update their used IPs and domains from time to time.'}, 'endNode': {'label': \"Scientific Article's Limitation of SOTA\", 'name': ''}, 'name': 'has limitation'}\n",
      "[INFO][INVENTED ENTITIES] Aie; the entities label='Scientific_Article_s_Limitation_of_SOTA' name='' properties=EntityProperties(embeddings=None) is invented. Solving it ...\n",
      "[INFO] Wohoo! Relation was matched --- [has_limitation] --merged --> [same_as] \n",
      "[INFO] ------- Extracting Entities from the Document 6\n",
      "[INFO] Wohoo! Entity was matched --- [threat intelligence knowledge graph construction:Technique] --merged--> [social engineering:Methodology]\n",
      "[INFO] Wohoo! Entity was matched --- [fine tuning llama 15000 model using a larger dataset and correcting misclassifications by increasing the number of certain types of entities in the training data:Methodology] --merged--> [social engineering:Methodology]\n",
      "[INFO] ------- Extracting Relations from the Document 6\n",
      "[INFO] Verification of invented entities\n",
      "[DEBUG] Processing relationship: {'startNode': {'label': 'Methodology', 'name': 'social engineering'}, 'endNode': {'label': 'Technique', 'name': 'mitre att&ck'}, 'name': 'uses'}\n",
      "[DEBUG] Processing relationship: {'startNode': {'label': 'Data Structure', 'name': 'neo4j graph database'}, 'endNode': {'label': 'Process', 'name': 'extracting and processing information from attack analysis reports'}, 'name': 'stores'}\n",
      "[DEBUG] Processing relationship: {'startNode': {'label': 'Person', 'name': 'experienced knowledge graph builder'}, 'endNode': {'label': 'Data Structure', 'name': 'neo4j graph database'}, 'name': 'uses'}\n",
      "[INFO] ------- Extracting Entities from the Document 7\n",
      "[INFO] Wohoo! Entity was matched --- [unknown:Attacker] --merged--> [attackers:Person]\n",
      "[INFO] ------- Extracting Relations from the Document 7\n",
      "[INFO] Verification of invented entities\n",
      "[DEBUG] Processing relationship: {'startNode': {'label': 'Tool', 'name': 'Cobalt Strike'}, 'endNode': {'label': 'Attacker', 'name': 'Person'}, 'name': 'Used By'}\n",
      "[INFO][INVENTED ENTITIES] Aie; the entities label='Attacker' name='person' properties=EntityProperties(embeddings=None) is invented. Solving it ...\n",
      "[INFO] Wohoo! Entity was matched --- [person:Attacker] --merged--> [attackers:Person]\n",
      "[DEBUG] Processing relationship: {'startNode': {'label': 'Tool', 'name': 'Mimikatz'}, 'endNode': {'label': 'Attacker', 'name': 'Person'}, 'name': 'Used By'}\n",
      "[INFO][INVENTED ENTITIES] Aie; the entities label='Attacker' name='person' properties=EntityProperties(embeddings=None) is invented. Solving it ...\n",
      "[INFO] Wohoo! Entity was matched --- [person:Attacker] --merged--> [attackers:Person]\n",
      "[DEBUG] Processing relationship: {'startNode': {'label': 'Tool', 'name': 'PowerShell'}, 'endNode': {'label': 'Attacker', 'name': 'Person'}, 'name': 'Used By'}\n",
      "[INFO][INVENTED ENTITIES] Aie; the entities label='Attacker' name='person' properties=EntityProperties(embeddings=None) is invented. Solving it ...\n",
      "[INFO] Wohoo! Entity was matched --- [person:Attacker] --merged--> [attackers:Person]\n",
      "[DEBUG] Processing relationship: {'startNode': {'label': 'MD5', 'name': 'hash function'}, 'endNode': {'label': 'Tool', 'name': 'Cobalt Strike'}, 'name': 'Hashed With'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nsuccess = False\\nattempts = 0\\n\\nwhile not success:\\n    try:\\n        kg = itext2kg.build_graph(sections=distilled_docs[0], ent_threshold=0.7, rel_threshold=0.7)\\n        success = True\\n    except Exception as e:\\n        attempts += 1\\n        print(f\"Attempt {attempts} failed with error: {e}\") \\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kg = itext2kg.build_graph(sections=distilled_docs[0], ent_threshold=0.7, rel_threshold=0.7)\n",
    "\"\"\"\n",
    "success = False\n",
    "attempts = 0\n",
    "\n",
    "while not success:\n",
    "    try:\n",
    "        kg = itext2kg.build_graph(sections=distilled_docs[0], ent_threshold=0.7, rel_threshold=0.7)\n",
    "        success = True\n",
    "    except Exception as e:\n",
    "        attempts += 1\n",
    "        print(f\"Attempt {attempts} failed with error: {e}\") \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We construct the second graph, noting that we already have an existing knowledge graph (for the first article)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] ------- Extracting Entities from the Document 1\n",
      "[INFO] ------- Extracting Relations from the Document 1\n",
      "[INFO] Verification of invented entities\n",
      "[DEBUG] Processing relationship: {'startNode': {'label': 'Data_Structure', 'name': 'Cyber threat intelligence report'}, 'endNode': {'label': 'Methodology', 'name': 'Few shot prompting and fine tuning'}, 'name': 'Uses'}\n",
      "[DEBUG] Processing relationship: {'startNode': {'label': 'Person', 'name': 'John Doe'}, 'endNode': {'label': 'Technique', 'name': 'Neural networks'}, 'name': 'Studies'}\n",
      "[INFO] ------- Extracting Entities from the Document 2\n",
      "[INFO] Wohoo! Entity was matched --- [entity types in the few shot examples or fine tuning data:Ontology] --merged--> [few shot prompting and fine tuning:Methodology]\n",
      "[INFO] ------- Extracting Relations from the Document 2\n",
      "[INFO] Verification of invented entities\n",
      "[DEBUG] Processing relationship: {'startNode': {'label': 'Fine_tuned_model', 'name': '7b chat model'}, 'endNode': {'label': 'Entity_types', 'name': 'entity types in the few-shot examples or fine-tuning data'}, 'name': 'Extracts more accurate triples than'}\n",
      "[INFO][INVENTED ENTITIES] Aie; the entities label='Entity_types' name='entity types in the few shot examples or fine tuning data' properties=EntityProperties(embeddings=None) is invented. Solving it ...\n",
      "[INFO] Wohoo! Entity was matched --- [entity types in the few shot examples or fine tuning data:Entity_types] --merged--> [model that performs well on a small scale test set:Guidance_model]\n",
      "[DEBUG] Processing relationship: {'startNode': {'label': 'Guidance_model', 'name': 'model that performs well on a small scale test set'}, 'endNode': {'label': 'Entity_types', 'name': 'entity types in the few-shot examples or fine-tuning data'}, 'name': 'Applying models to a large-scale dataset can yield poor results with a lot of noise'}\n",
      "[INFO][INVENTED ENTITIES] Aie; the entities label='Entity_types' name='entity types in the few shot examples or fine tuning data' properties=EntityProperties(embeddings=None) is invented. Solving it ...\n",
      "[INFO] Wohoo! Entity was matched --- [entity types in the few shot examples or fine tuning data:Entity_types] --merged--> [model that performs well on a small scale test set:Guidance_model]\n",
      "[DEBUG] Processing relationship: {'startNode': {'label': 'Methodology', 'name': 'few shot prompting and fine tuning'}, 'endNode': {'label': 'Entity_types', 'name': 'entity types in the few-shot examples or fine-tuning data'}, 'name': 'To adhere to an ontology for triple extraction, including entity types is necessary'}\n",
      "[INFO][INVENTED ENTITIES] Aie; the entities label='Entity_types' name='entity types in the few shot examples or fine tuning data' properties=EntityProperties(embeddings=None) is invented. Solving it ...\n",
      "[INFO] Wohoo! Entity was matched --- [entity types in the few shot examples or fine tuning data:Entity_types] --merged--> [model that performs well on a small scale test set:Guidance_model]\n",
      "[INFO] ------- Extracting Entities from the Document 3\n",
      "[INFO] ------- Extracting Relations from the Document 3\n",
      "[INFO] Verification of invented entities\n",
      "[DEBUG] Processing relationship: {'startNode': {'label': 'Scalability_limitation', 'name': 'existing link prediction models in knowledge graphs are not scalable for large scale datasets'}, 'endNode': {'label': 'Limitation_of_existing_work', 'name': 'The existing work (LADDER) has limitations when it comes to extracting triples from a large-scale dataset.'}, 'name': 'has limitation'}\n",
      "[INFO][INVENTED ENTITIES] Aie; the entities label='Limitation_of_existing_work' name='the existing work (ladder) has limitations when it comes to extracting triples from a large scale dataset.' properties=EntityProperties(embeddings=None) is invented. Solving it ...\n",
      "[INFO] Wohoo! Entity was matched --- [the existing work (ladder) has limitations when it comes to extracting triples from a large scale dataset.:Limitation_of_existing_work] --merged--> [incorrectly connecting entity types with certain relationships and extracting triples with newly invented entity types or relationships:Limitation_of_existing_work]\n",
      "[DEBUG] Processing relationship: {'startNode': {'label': 'Interpretability_limitation', 'name': 'existing link prediction models in knowledge graphs lack interpretability, making it difficult to understand the reasoning behind their predictions'}, 'endNode': {'label': 'Limitation_of_existing_work', 'name': 'The existing work (LADDER) has limitations when it comes to extracting triples from a large-scale dataset.'}, 'name': 'has limitation'}\n",
      "[INFO][INVENTED ENTITIES] Aie; the entities label='Limitation_of_existing_work' name='the existing work (ladder) has limitations when it comes to extracting triples from a large scale dataset.' properties=EntityProperties(embeddings=None) is invented. Solving it ...\n",
      "[INFO] Wohoo! Entity was matched --- [the existing work (ladder) has limitations when it comes to extracting triples from a large scale dataset.:Limitation_of_existing_work] --merged--> [incorrectly connecting entity types with certain relationships and extracting triples with newly invented entity types or relationships:Limitation_of_existing_work]\n",
      "[DEBUG] Processing relationship: {'startNode': {'label': 'Limitation_of_existing_work', 'name': 'incorrectly connecting entity types with certain relationships and extracting triples with newly invented entity types or relationships'}, 'endNode': {'label': 'Scalability_limitation', 'name': 'existing link prediction models in knowledge graphs are not scalable for large scale datasets'}, 'name': 'has limitation'}\n",
      "[INFO] ------- Extracting Entities from the Document 4\n",
      "[INFO] Wohoo! Entity was matched --- [link prediction:Methodology] --merged--> [neural networks:Technique]\n",
      "[INFO] ------- Extracting Relations from the Document 4\n",
      "[INFO] Verification of invented entities\n",
      "[DEBUG] Processing relationship: {'startNode': {'label': 'Technique', 'name': 'Neural Networks'}, 'endNode': {'label': 'Data_Structure', 'name': 'Triple Extraction'}, 'name': 'uses'}\n",
      "[DEBUG] Processing relationship: {'startNode': {'label': 'Technique', 'name': 'Neural Networks'}, 'endNode': {'label': 'Technique', 'name': 'Neural Networks'}, 'name': 'is_fine_tuned_by'}\n",
      "[INFO] Wohoo! Relation was matched --- [uses] --merged --> [Uses] \n",
      "[INFO] ------- Extracting Entities from the Document 5\n",
      "[INFO] Wohoo! Entity was matched --- [mitre att&ck techniques:Technique] --merged--> [neural networks:Technique]\n",
      "[INFO] ------- Extracting Relations from the Document 5\n",
      "[INFO] Verification of invented entities\n",
      "[DEBUG] Processing relationship: {'startNode': {'label': 'Scalability', 'name': 'Improving Scalability'}, 'endNode': {'label': 'Data_Quality', 'name': 'High Quality Training Data'}, 'name': 'Requires'}\n",
      "[DEBUG] Processing relationship: {'startNode': {'label': 'Limitation', 'name': 'Performance Limitation'}, 'endNode': {'label': 'Technique', 'name': 'Neural Networks'}, 'name': 'Uses'}\n",
      "[DEBUG] Processing relationship: {'startNode': {'label': 'Interpretability', 'name': 'Improving Interpretability'}, 'endNode': {'label': 'Data_Quality', 'name': 'High Quality Training Data'}, 'name': 'Requires'}\n",
      "[INFO] ------- Matching the Document 1 Entities and Relationships with the Existing Global Entities/Relations\n",
      "[INFO] Wohoo! Entity was matched --- [existing link prediction models in knowledge graphs lack interpretability, making it difficult to understand the reasoning behind their predictions:Interpretability_limitation] --merged--> [the limitations of the current state of the art (sota) methods or techniques:Limitation_of_SOTA]\n",
      "[INFO] Wohoo! Entity was matched --- [incorrectly connecting entity types with certain relationships and extracting triples with newly invented entity types or relationships:Limitation_of_existing_work] --merged--> [the limitations of the current state of the art (sota) methods or techniques:Limitation_of_SOTA]\n",
      "[INFO] Wohoo! Entity was matched --- [existing link prediction models in knowledge graphs are not scalable for large scale datasets:Scalability_limitation] --merged--> [the limitations of the current state of the art (sota) methods or techniques:Limitation_of_SOTA]\n",
      "[INFO] Wohoo! Entity was matched --- [performance limitation:Limitation] --merged--> [existing intrusion detection methods that rely solely on ioc values are ineffective in detecting attacks, as attackers often update their used ips and domains from time to time.:Limitation]\n",
      "[INFO] Wohoo! Entity was matched --- [john doe:Person] --merged--> [attackers:Person]\n",
      "[INFO] Wohoo! Entity was matched --- [high quality training data:Data_Quality] --merged--> [identification of malicious samples for training models:Malicious_Samples]\n",
      "[INFO] Wohoo! Entity was matched --- [cyber threat intelligence report:Data_Structure] --merged--> [9681 articles classified as threat intelligence reports:Threat_Intelligence_Report]\n",
      "[INFO] Wohoo! Entity was matched --- [few shot prompting and fine tuning:Methodology] --merged--> [social engineering:Methodology]\n",
      "[INFO] Wohoo! Entity was matched --- [neural networks:Technique] --merged--> [social engineering:Methodology]\n",
      "[INFO] Wohoo! Relation was matched --- [Uses] --merged --> [uses] \n",
      "[INFO] Wohoo! Relation was matched --- [has_limitation] --merged --> [same_as] \n",
      "[INFO] Wohoo! Relation was matched --- [has_limitation] --merged --> [same_as] \n",
      "[INFO] Wohoo! Relation was matched --- [has_limitation] --merged --> [same_as] \n",
      "[INFO] Wohoo! Relation was matched --- [Uses] --merged --> [uses] \n",
      "[INFO] Wohoo! Relation was matched --- [Uses] --merged --> [uses] \n"
     ]
    }
   ],
   "source": [
    "kg2 = itext2kg.build_graph(sections=distilled_docs[1], existing_knowledge_graph=kg, rel_threshold=0.7, ent_threshold=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draw the graph\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final section involves visualizing the constructed knowledge graph using GraphIntegrator. The graph database Neo4j is accessed using specified credentials, and the resulting graph is visualized to provide a visual representation of the relationships and entities extracted from the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ServiceUnavailable",
     "evalue": "Couldn't connect to localhost:7687 (resolved to ()):\nFailed to establish connection to ResolvedIPv4Address(('127.0.0.1', 7687)) (reason [WinError 10061] 由于目标计算机积极拒绝，无法连接。)\nFailed to establish connection to ResolvedIPv6Address(('::1', 7687, 0, 0)) (reason [WinError 10061] 由于目标计算机积极拒绝，无法连接。)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectionRefusedError\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\soft\\anaconda3\\envs\\itext2kgenv\\Lib\\site-packages\\neo4j\\_async_compat\\network\\_bolt_socket.py:528\u001b[39m, in \u001b[36mBoltSocket._connect\u001b[39m\u001b[34m(cls, resolved_address, timeout, keep_alive)\u001b[39m\n\u001b[32m    527\u001b[39m log.debug(\u001b[33m\"\u001b[39m\u001b[33m[#0000]  C: <OPEN> \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, resolved_address)\n\u001b[32m--> \u001b[39m\u001b[32m528\u001b[39m \u001b[43ms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresolved_address\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    529\u001b[39m s.settimeout(t)\n",
      "\u001b[31mConnectionRefusedError\u001b[39m: [WinError 10061] 由于目标计算机积极拒绝，无法连接。",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mServiceUnavailable\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\soft\\anaconda3\\envs\\itext2kgenv\\Lib\\site-packages\\neo4j\\_async_compat\\network\\_bolt_socket.py:690\u001b[39m, in \u001b[36mBoltSocket.connect\u001b[39m\u001b[34m(cls, address, tcp_timeout, deadline, custom_resolver, ssl_context, keep_alive)\u001b[39m\n\u001b[32m    689\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m690\u001b[39m     s = \u001b[43mBoltSocket\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresolved_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtcp_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    691\u001b[39m \u001b[43m                            \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    692\u001b[39m     s = BoltSocket._secure(s, resolved_address._host_name,\n\u001b[32m    693\u001b[39m                            ssl_context)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\soft\\anaconda3\\envs\\itext2kgenv\\Lib\\site-packages\\neo4j\\_async_compat\\network\\_bolt_socket.py:546\u001b[39m, in \u001b[36mBoltSocket._connect\u001b[39m\u001b[34m(cls, resolved_address, timeout, keep_alive)\u001b[39m\n\u001b[32m    545\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(error, \u001b[38;5;167;01mOSError\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m546\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ServiceUnavailable(\n\u001b[32m    547\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFailed to establish connection to \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[33m (reason \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    548\u001b[39m         .format(resolved_address, error)\n\u001b[32m    549\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merror\u001b[39;00m\n\u001b[32m    550\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mServiceUnavailable\u001b[39m: Failed to establish connection to ResolvedIPv4Address(('127.0.0.1', 7687)) (reason [WinError 10061] 由于目标计算机积极拒绝，无法连接。)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mServiceUnavailable\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m USERNAME = \u001b[33m\"\u001b[39m\u001b[33mneo4j\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      6\u001b[39m PASSWORD = \u001b[33m\"\u001b[39m\u001b[33mLHSqaz123\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43mGraphIntegrator\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m=\u001b[49m\u001b[43mURI\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musername\u001b[49m\u001b[43m=\u001b[49m\u001b[43mUSERNAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPASSWORD\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvisualize_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mknowledge_graph\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkg2\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\soft\\anaconda3\\envs\\itext2kgenv\\Lib\\site-packages\\itext2kg\\graph_integration\\graph_integrator.py:143\u001b[39m, in \u001b[36mGraphIntegrator.visualize_graph\u001b[39m\u001b[34m(self, knowledge_graph)\u001b[39m\n\u001b[32m    137\u001b[39m nodes, relationships = (\n\u001b[32m    138\u001b[39m     \u001b[38;5;28mself\u001b[39m.create_nodes(knowledge_graph=knowledge_graph),\n\u001b[32m    139\u001b[39m     \u001b[38;5;28mself\u001b[39m.create_relationships(knowledge_graph=knowledge_graph),\n\u001b[32m    140\u001b[39m )\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m nodes:\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m relation \u001b[38;5;129;01min\u001b[39;00m relationships:\n\u001b[32m    146\u001b[39m     \u001b[38;5;28mself\u001b[39m.run_query(relation)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\soft\\anaconda3\\envs\\itext2kgenv\\Lib\\site-packages\\itext2kg\\graph_integration\\graph_integrator.py:43\u001b[39m, in \u001b[36mGraphIntegrator.run_query\u001b[39m\u001b[34m(self, query)\u001b[39m\n\u001b[32m     41\u001b[39m session = \u001b[38;5;28mself\u001b[39m.driver.session()\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m     \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     45\u001b[39m     session.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\soft\\anaconda3\\envs\\itext2kgenv\\Lib\\site-packages\\neo4j\\_sync\\work\\session.py:302\u001b[39m, in \u001b[36mSession.run\u001b[39m\u001b[34m(self, query, parameters, **kwargs)\u001b[39m\n\u001b[32m    299\u001b[39m     \u001b[38;5;28mself\u001b[39m._auto_result._buffer_all()\n\u001b[32m    301\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection:\n\u001b[32m--> \u001b[39m\u001b[32m302\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdefault_access_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    303\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    304\u001b[39m cx = \u001b[38;5;28mself\u001b[39m._connection\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\soft\\anaconda3\\envs\\itext2kgenv\\Lib\\site-packages\\neo4j\\_sync\\work\\session.py:130\u001b[39m, in \u001b[36mSession._connect\u001b[39m\u001b[34m(self, access_mode, **acquire_kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m     access_mode = \u001b[38;5;28mself\u001b[39m._config.default_access_mode\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[43m        \u001b[49m\u001b[43maccess_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43macquire_kwargs\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m asyncio.CancelledError:\n\u001b[32m    134\u001b[39m     \u001b[38;5;28mself\u001b[39m._handle_cancellation(message=\u001b[33m\"\u001b[39m\u001b[33m_connect\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\soft\\anaconda3\\envs\\itext2kgenv\\Lib\\site-packages\\neo4j\\_sync\\work\\workspace.py:182\u001b[39m, in \u001b[36mWorkspace._connect\u001b[39m\u001b[34m(self, access_mode, auth, **acquire_kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m acquire_kwargs_ = {\n\u001b[32m    174\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33maccess_mode\u001b[39m\u001b[33m\"\u001b[39m: access_mode,\n\u001b[32m    175\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: acquisition_timeout,\n\u001b[32m   (...)\u001b[39m\u001b[32m    179\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mliveness_check_timeout\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    180\u001b[39m }\n\u001b[32m    181\u001b[39m acquire_kwargs_.update(acquire_kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m \u001b[38;5;28mself\u001b[39m._connection = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43macquire_kwargs_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[38;5;28mself\u001b[39m._connection_access_mode = access_mode\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\soft\\anaconda3\\envs\\itext2kgenv\\Lib\\site-packages\\neo4j\\_sync\\io\\_pool.py:526\u001b[39m, in \u001b[36mBoltPool.acquire\u001b[39m\u001b[34m(self, access_mode, timeout, database, bookmarks, auth, liveness_check_timeout)\u001b[39m\n\u001b[32m    523\u001b[39m log.debug(\u001b[33m\"\u001b[39m\u001b[33m[#0000]  _: <POOL> acquire direct connection, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    524\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33maccess_mode=\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m, database=\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m, access_mode, database)\n\u001b[32m    525\u001b[39m deadline = Deadline.from_timeout_or_deadline(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m526\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_acquire\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    527\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mliveness_check_timeout\u001b[49m\n\u001b[32m    528\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\soft\\anaconda3\\envs\\itext2kgenv\\Lib\\site-packages\\neo4j\\_sync\\io\\_pool.py:313\u001b[39m, in \u001b[36mIOPool._acquire\u001b[39m\u001b[34m(self, address, auth, deadline, liveness_check_timeout)\u001b[39m\n\u001b[32m    308\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m ClientError(\n\u001b[32m    309\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfailed to obtain a connection from the pool within \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    310\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[33ms (timeout)\u001b[39m\u001b[33m\"\u001b[39m.format(deadline.original_timeout)\n\u001b[32m    311\u001b[39m             )\n\u001b[32m    312\u001b[39m log.debug(\u001b[33m\"\u001b[39m\u001b[33m[#0000]  _: <POOL> trying to hand out new connection\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m313\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconnection_creator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\soft\\anaconda3\\envs\\itext2kgenv\\Lib\\site-packages\\neo4j\\_sync\\io\\_pool.py:163\u001b[39m, in \u001b[36mIOPool._acquire_new_later.<locals>.connection_creator\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    162\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m         connection = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mopener\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m            \u001b[49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpool_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeadline\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m ServiceUnavailable:\n\u001b[32m    167\u001b[39m         \u001b[38;5;28mself\u001b[39m.deactivate(address)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\soft\\anaconda3\\envs\\itext2kgenv\\Lib\\site-packages\\neo4j\\_sync\\io\\_pool.py:500\u001b[39m, in \u001b[36mBoltPool.open.<locals>.opener\u001b[39m\u001b[34m(addr, auth_manager, deadline)\u001b[39m\n\u001b[32m    499\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mopener\u001b[39m(addr, auth_manager, deadline):\n\u001b[32m--> \u001b[39m\u001b[32m500\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBolt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m        \u001b[49m\u001b[43maddr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrouting_context\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpool_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpool_config\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\soft\\anaconda3\\envs\\itext2kgenv\\Lib\\site-packages\\neo4j\\_sync\\io\\_bolt.py:405\u001b[39m, in \u001b[36mBolt.open\u001b[39m\u001b[34m(cls, address, auth_manager, deadline, routing_context, pool_config)\u001b[39m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m deadline \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    402\u001b[39m     deadline = Deadline(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    404\u001b[39m s, protocol_version, handshake, data = \\\n\u001b[32m--> \u001b[39m\u001b[32m405\u001b[39m     \u001b[43mBoltSocket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m        \u001b[49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtcp_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpool_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnection_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcustom_resolver\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpool_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresolver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m        \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpool_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_ssl_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpool_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    414\u001b[39m pool_config.protocol_version = protocol_version\n\u001b[32m    416\u001b[39m \u001b[38;5;66;03m# Carry out Bolt subclass imports locally to avoid circular dependency\u001b[39;00m\n\u001b[32m    417\u001b[39m \u001b[38;5;66;03m# issues.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\soft\\anaconda3\\envs\\itext2kgenv\\Lib\\site-packages\\neo4j\\_async_compat\\network\\_bolt_socket.py:718\u001b[39m, in \u001b[36mBoltSocket.connect\u001b[39m\u001b[34m(cls, address, tcp_timeout, deadline, custom_resolver, ssl_context, keep_alive)\u001b[39m\n\u001b[32m    713\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ServiceUnavailable(\n\u001b[32m    714\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt connect to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m (resolved to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m % (\n\u001b[32m    715\u001b[39m             \u001b[38;5;28mstr\u001b[39m(address), \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m, resolved_addresses)))\n\u001b[32m    716\u001b[39m     )\n\u001b[32m    717\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m718\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ServiceUnavailable(\n\u001b[32m    719\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt connect to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m (resolved to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m % (\n\u001b[32m    720\u001b[39m             \u001b[38;5;28mstr\u001b[39m(address), \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m, resolved_addresses)),\n\u001b[32m    721\u001b[39m             \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m, errors))\n\u001b[32m    722\u001b[39m         )\n\u001b[32m    723\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merrors\u001b[39;00m[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mServiceUnavailable\u001b[39m: Couldn't connect to localhost:7687 (resolved to ()):\nFailed to establish connection to ResolvedIPv4Address(('127.0.0.1', 7687)) (reason [WinError 10061] 由于目标计算机积极拒绝，无法连接。)\nFailed to establish connection to ResolvedIPv6Address(('::1', 7687, 0, 0)) (reason [WinError 10061] 由于目标计算机积极拒绝，无法连接。)"
     ]
    }
   ],
   "source": [
    "from itext2kg.graph_integration import GraphIntegrator\n",
    "\n",
    "\n",
    "URI = \"bolt://localhost:7687\"\n",
    "USERNAME = \"neo4j\"\n",
    "PASSWORD = \"your_password\"\n",
    "\n",
    "GraphIntegrator(uri=URI, username=USERNAME, password=PASSWORD).visualize_graph(knowledge_graph=kg2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "itext2kgpro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
